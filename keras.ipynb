{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and check GPU\n",
    "An Ampere architecture GPU (RTX 30xx) was used, which requires specific installation packages.\n",
    "\n",
    "1. NVIDIA Cuda Toolkit 11.2\n",
    "2. cuDNN 8.1.1\n",
    "3. Visual studio windows\n",
    "4. Python 3.8.6\n",
    "5. tf-nightly build (2.5.0dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Conv3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print('keras: %s' % keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation=tf.keras.layers.LeakyReLU(alpha=0.05)\n",
    "#activation='relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "layers = tf.keras.layers\n",
    "\n",
    "model.add(layers.Conv3D(filters=32, kernel_size=(4,4,4), strides=2,padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.05), use_bias=False, input_shape=(32,32,32,1),data_format='channels_last'))\n",
    "model.add(layers.Conv3D(filters=32, kernel_size=(4,4,4), strides=2,padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.05), use_bias=False))\n",
    "model.add(layers.Conv3D(filters=32, kernel_size=(4,4,4), strides=2,padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.05), use_bias=False))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64,tf.keras.layers.LeakyReLU(alpha=0.05),use_bias = False))\n",
    "model.add(layers.Dense(10,tf.keras.layers.LeakyReLU(alpha=0.05),use_bias = False))\n",
    "model.add(layers.Dense(1,tf.keras.layers.LeakyReLU(alpha=0.05),use_bias = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 16, 16, 16, 32)    2048      \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 8, 8, 8, 32)       65536     \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 4, 4, 4, 32)       65536     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                131072    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                640       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 264,842\n",
      "Trainable params: 264,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true,y_predicted):\n",
    "    error = 100*tf.math.abs((y_true-y_predicted)/y_true)*tf.math.tanh(30*y_true)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error as custom_loss or 'mape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001,amsgrad=True),loss='mape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Large `.mat` files saved with `'-v7.3'` do not work with loadmat. \n",
    "\n",
    "Mat73 must be installed through `pip install mat73`.\n",
    "\n",
    "Reshaping was done in `dataprocessing.ipynb` and the resulting 2.400.000x32x32x32x1 matrix is saved as `samples.npy`. This was split up into  segments (samples0-7) that fit in the RAM (32GB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of Epoch 1-1\n",
      "\n",
      "14985/14985 [==============================] - 68s 4ms/step - loss: 38.5723 - val_loss: 29.9979\n",
      "14985/14985 [==============================] - 74s 5ms/step - loss: 29.0987 - val_loss: 25.8864\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 26.3602 - val_loss: 25.6708\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 24.2770 - val_loss: 21.9970\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 22.6956 - val_loss: 24.8670\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 21.4006 - val_loss: 21.4177\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 20.6115 - val_loss: 19.5054\n",
      "9990/9990 [==============================] - 48s 5ms/step - loss: 20.0163 - val_loss: 17.6842\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 19.0761\n",
      "Set of Epoch 2-2\n",
      "\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 19.6152 - val_loss: 19.4349\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 18.9803 - val_loss: 16.7175\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 18.4953 - val_loss: 18.3085\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 18.1791 - val_loss: 17.5537\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 17.7536 - val_loss: 20.7708\n",
      "14985/14985 [==============================] - 74s 5ms/step - loss: 17.5957 - val_loss: 17.8132\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 17.2608 - val_loss: 16.8083\n",
      "9990/9990 [==============================] - 42s 4ms/step - loss: 17.1121 - val_loss: 18.6432\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 18.7388\n",
      "Set of Epoch 3-3\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 16.9764 - val_loss: 17.5881\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 16.6994 - val_loss: 15.5103\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 16.5052 - val_loss: 15.9079\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 16.4261 - val_loss: 17.3397\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 16.2211 - val_loss: 16.9563\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 16.1703 - val_loss: 17.6999\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 16.0050 - val_loss: 16.5091\n",
      "9990/9990 [==============================] - 42s 4ms/step - loss: 15.9539 - val_loss: 15.5702\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 16.0128\n",
      "Set of Epoch 4-4\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 15.9034 - val_loss: 15.3462\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 15.7064 - val_loss: 15.1266\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 15.5904 - val_loss: 15.1186\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 15.5741 - val_loss: 15.9474\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 15.4300 - val_loss: 16.2228\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 15.3929 - val_loss: 16.1868\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 15.3161 - val_loss: 15.1034\n",
      "9990/9990 [==============================] - 42s 4ms/step - loss: 15.2095 - val_loss: 17.2995\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 17.3223\n",
      "Set of Epoch 5-5\n",
      "\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 15.2418 - val_loss: 16.1185\n",
      "14985/14985 [==============================] - 74s 5ms/step - loss: 15.0995 - val_loss: 13.2289\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 15.0532 - val_loss: 14.9848\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 15.0578 - val_loss: 15.8130\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 14.9025 - val_loss: 16.0153\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 14.8545 - val_loss: 15.8702\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 14.8075 - val_loss: 15.3769\n",
      "9990/9990 [==============================] - 48s 5ms/step - loss: 14.7978 - val_loss: 14.0587\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 15.3664\n",
      "Set of Epoch 6-6\n",
      "\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 14.8168 - val_loss: 15.3892\n",
      "14985/14985 [==============================] - 74s 5ms/step - loss: 14.6535 - val_loss: 13.9356\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 14.6196 - val_loss: 14.3623\n",
      "14985/14985 [==============================] - 81s 5ms/step - loss: 14.6691 - val_loss: 15.6720\n",
      "14985/14985 [==============================] - 71s 5ms/step - loss: 14.5551 - val_loss: 16.6289\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 14.5187 - val_loss: 15.2387\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 14.5186 - val_loss: 14.8357\n",
      "9990/9990 [==============================] - 45s 4ms/step - loss: 14.4826 - val_loss: 14.2738\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 14.6976\n",
      "Set of Epoch 7-7\n",
      "\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 14.4897 - val_loss: 14.7839\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 14.3669 - val_loss: 12.5361\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 14.3246 - val_loss: 14.7126\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 14.3568 - val_loss: 15.0443\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 14.2582 - val_loss: 16.1188\n",
      "14985/14985 [==============================] - 74s 5ms/step - loss: 14.2555 - val_loss: 15.6562\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 14.2610 - val_loss: 15.2147\n",
      "9990/9990 [==============================] - 44s 4ms/step - loss: 14.1618 - val_loss: 13.8245\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 14.6079\n",
      "Set of Epoch 8-8\n",
      "\n",
      "14985/14985 [==============================] - 70s 5ms/step - loss: 14.1768 - val_loss: 14.4518\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 14.1409 - val_loss: 12.9383\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 14.1329 - val_loss: 13.0950\n",
      "14985/14985 [==============================] - 79s 5ms/step - loss: 14.0664 - val_loss: 15.2872\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 14.0207 - val_loss: 16.3709\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 13.9965 - val_loss: 16.2421\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 14.0281 - val_loss: 14.1950\n",
      "9990/9990 [==============================] - 44s 4ms/step - loss: 13.9688 - val_loss: 14.6753\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 15.3377\n",
      "Set of Epoch 9-9\n",
      "\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 14.0090 - val_loss: 13.8118\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 13.9217 - val_loss: 13.8113\n",
      "14985/14985 [==============================] - 64s 4ms/step - loss: 13.8862 - val_loss: 13.6096\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 13.8763 - val_loss: 15.8497\n",
      "14985/14985 [==============================] - 69s 5ms/step - loss: 13.8153 - val_loss: 15.0538\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 13.8616 - val_loss: 14.3714\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 13.8131 - val_loss: 15.2618\n",
      "9990/9990 [==============================] - 44s 4ms/step - loss: 13.7771 - val_loss: 15.1546\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 15.5544\n",
      "Set of Epoch 10-10\n",
      "\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 13.8219 - val_loss: 14.0930\n",
      "14985/14985 [==============================] - 82s 5ms/step - loss: 13.7186 - val_loss: 12.7061\n",
      "14985/14985 [==============================] - 71s 5ms/step - loss: 13.7611 - val_loss: 13.7444\n",
      "14985/14985 [==============================] - 82s 5ms/step - loss: 13.7686 - val_loss: 16.2015\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 13.6797 - val_loss: 15.6326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14985/14985 [==============================] - 78s 5ms/step - loss: 13.6916 - val_loss: 15.9067\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 13.6878 - val_loss: 14.2151\n",
      "9990/9990 [==============================] - 43s 4ms/step - loss: 13.5812 - val_loss: 14.0396\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 14.7064\n",
      "Set of Epoch 11-11\n",
      "\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 13.6123 - val_loss: 14.6398\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 13.5720 - val_loss: 12.7282\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 13.5711 - val_loss: 13.5146\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 13.5716 - val_loss: 15.4365\n",
      "14985/14985 [==============================] - 64s 4ms/step - loss: 13.5102 - val_loss: 15.2713\n",
      "14985/14985 [==============================] - 75s 5ms/step - loss: 13.5836 - val_loss: 14.1247\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 13.5295 - val_loss: 15.3017\n",
      "9990/9990 [==============================] - 45s 5ms/step - loss: 13.4832 - val_loss: 13.6089\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 13.9632\n",
      "Set of Epoch 12-12\n",
      "\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 13.4856 - val_loss: 13.8545\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 13.3980 - val_loss: 12.8766\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 13.4083 - val_loss: 13.3970\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 13.4454 - val_loss: 16.8633\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 13.3986 - val_loss: 15.2848\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 13.4114 - val_loss: 13.7556\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 13.3837 - val_loss: 13.8885\n",
      "9990/9990 [==============================] - 44s 4ms/step - loss: 13.3775 - val_loss: 13.2469\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 14.0098\n",
      "Set of Epoch 13-13\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.3647 - val_loss: 15.0313\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 13.2917 - val_loss: 14.6947\n",
      "14985/14985 [==============================] - 64s 4ms/step - loss: 13.2982 - val_loss: 13.7011\n",
      "14985/14985 [==============================] - 82s 5ms/step - loss: 13.3231 - val_loss: 15.2230\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 13.3091 - val_loss: 14.9473\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 13.2773 - val_loss: 13.8622\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.3366 - val_loss: 13.8190\n",
      "9990/9990 [==============================] - 42s 4ms/step - loss: 13.2310 - val_loss: 12.8524\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 14.0831\n",
      "Set of Epoch 14-14\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.3748 - val_loss: 13.9177\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 13.2243 - val_loss: 12.5234\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 13.1910 - val_loss: 13.6656\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 13.2196 - val_loss: 14.6562\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.1457 - val_loss: 16.1560\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 13.1440 - val_loss: 13.8283\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.1446 - val_loss: 14.1780\n",
      "9990/9990 [==============================] - 41s 4ms/step - loss: 13.0742 - val_loss: 12.3749\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 13.8640\n",
      "Set of Epoch 15-15\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.1803 - val_loss: 13.2605\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 13.0917 - val_loss: 12.4546\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 13.0401 - val_loss: 13.2420\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 13.1343 - val_loss: 16.1773\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.0455 - val_loss: 15.3528\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 13.0660 - val_loss: 14.1256\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.0940 - val_loss: 13.3297\n",
      "9990/9990 [==============================] - 42s 4ms/step - loss: 13.0393 - val_loss: 13.8925\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 14.6847\n",
      "Set of Epoch 16-16\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 13.0558 - val_loss: 13.6745\n",
      "14985/14985 [==============================] - 71s 5ms/step - loss: 13.0244 - val_loss: 12.2141\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 13.0356 - val_loss: 14.7184\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 13.0372 - val_loss: 15.7918\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 12.9916 - val_loss: 15.0149\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.9790 - val_loss: 15.1972\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 13.1559 - val_loss: 14.4727\n",
      "9990/9990 [==============================] - 44s 4ms/step - loss: 12.9270 - val_loss: 12.3639\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 13.7254\n",
      "Set of Epoch 17-17\n",
      "\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.9801 - val_loss: 12.9522\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 12.8768 - val_loss: 11.7877\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.8653 - val_loss: 14.4849\n",
      "14985/14985 [==============================] - 84s 6ms/step - loss: 12.9210 - val_loss: 14.0785\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 12.8372 - val_loss: 14.6074\n",
      "14985/14985 [==============================] - 84s 6ms/step - loss: 12.8541 - val_loss: 14.3838\n",
      "14985/14985 [==============================] - 70s 5ms/step - loss: 12.8963 - val_loss: 13.6079\n",
      "9990/9990 [==============================] - 47s 5ms/step - loss: 12.7821 - val_loss: 13.0554\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 13.6860\n",
      "Set of Epoch 18-18\n",
      "\n",
      "14985/14985 [==============================] - 69s 5ms/step - loss: 12.8449 - val_loss: 13.2356\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.8860 - val_loss: 12.8106\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 12.8116 - val_loss: 13.4608\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 12.8255 - val_loss: 14.9009\n",
      "14985/14985 [==============================] - 70s 5ms/step - loss: 12.7462 - val_loss: 14.8139\n",
      "14985/14985 [==============================] - 76s 5ms/step - loss: 12.7785 - val_loss: 13.9652\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 12.8263 - val_loss: 13.8416\n",
      "9990/9990 [==============================] - 43s 4ms/step - loss: 12.7839 - val_loss: 12.2164\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 14.0522\n",
      "Set of Epoch 19-19\n",
      "\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 12.8276 - val_loss: 14.1078\n",
      "14985/14985 [==============================] - 81s 5ms/step - loss: 12.6986 - val_loss: 12.3788\n",
      "14985/14985 [==============================] - 69s 5ms/step - loss: 12.7024 - val_loss: 13.1871\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.7635 - val_loss: 14.9304\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 12.7333 - val_loss: 14.9343\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.7181 - val_loss: 14.5414\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 12.7413 - val_loss: 14.4720\n",
      "9990/9990 [==============================] - 44s 4ms/step - loss: 12.6953 - val_loss: 11.4425\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 13.5542\n",
      "Set of Epoch 20-20\n",
      "\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 12.7163 - val_loss: 14.1156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14985/14985 [==============================] - 76s 5ms/step - loss: 12.6431 - val_loss: 11.7000\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 12.6479 - val_loss: 13.2583\n",
      "14985/14985 [==============================] - 81s 5ms/step - loss: 12.6921 - val_loss: 14.3841\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.6653 - val_loss: 15.4114\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 12.6588 - val_loss: 14.0773\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.7801 - val_loss: 14.1738\n",
      "9990/9990 [==============================] - 45s 4ms/step - loss: 12.6690 - val_loss: 11.8754\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 14.3250\n",
      "Set of Epoch 21-21\n",
      "\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.6828 - val_loss: 12.5171\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.5922 - val_loss: 11.6476\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 12.5714 - val_loss: 13.8195\n",
      "14985/14985 [==============================] - 78s 5ms/step - loss: 12.6583 - val_loss: 14.8819\n",
      "14985/14985 [==============================] - 66s 4ms/step - loss: 12.5934 - val_loss: 14.6194\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.5954 - val_loss: 13.9719\n",
      "14985/14985 [==============================] - 64s 4ms/step - loss: 12.7204 - val_loss: 14.4699\n",
      "9990/9990 [==============================] - 43s 4ms/step - loss: 12.6342 - val_loss: 12.2750\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 13.4780\n",
      "Set of Epoch 22-22\n",
      "\n",
      "14985/14985 [==============================] - 64s 4ms/step - loss: 12.5850 - val_loss: 13.9647\n",
      "14985/14985 [==============================] - 75s 5ms/step - loss: 12.5970 - val_loss: 11.4375\n",
      "14985/14985 [==============================] - 64s 4ms/step - loss: 12.5098 - val_loss: 13.2192\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.6035 - val_loss: 14.3477\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.5420 - val_loss: 14.5237\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.6005 - val_loss: 13.9700\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.6202 - val_loss: 13.3612\n",
      "9990/9990 [==============================] - 41s 4ms/step - loss: 12.4993 - val_loss: 13.8811\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 15.2762\n",
      "Set of Epoch 23-23\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.5104 - val_loss: 14.2381\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.4455 - val_loss: 12.4942\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.4686 - val_loss: 13.7852\n",
      "14985/14985 [==============================] - 71s 5ms/step - loss: 12.5092 - val_loss: 14.3098\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.5028 - val_loss: 15.1599\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 12.5219 - val_loss: 13.8052\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.4997 - val_loss: 13.8209\n",
      "9990/9990 [==============================] - 42s 4ms/step - loss: 12.4703 - val_loss: 12.1101\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 13.6346\n",
      "Set of Epoch 24-24\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.4847 - val_loss: 13.0354\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.4169 - val_loss: 13.0672\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.4246 - val_loss: 12.8566\n",
      "14985/14985 [==============================] - 71s 5ms/step - loss: 12.6534 - val_loss: 15.2403\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.3535 - val_loss: 14.7231\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 12.3762 - val_loss: 13.6222\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.4672 - val_loss: 15.1632\n",
      "9990/9990 [==============================] - 41s 4ms/step - loss: 12.3762 - val_loss: 11.5343\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 13.3765\n",
      "Set of Epoch 25-25\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.3743 - val_loss: 12.5134\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.2838 - val_loss: 11.5879\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 12.2884 - val_loss: 12.7731\n",
      "14985/14985 [==============================] - 71s 5ms/step - loss: 12.5411 - val_loss: 14.5146\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.3042 - val_loss: 14.6410\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.2974 - val_loss: 15.4445\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.3799 - val_loss: 14.1210\n",
      "9990/9990 [==============================] - 41s 4ms/step - loss: 12.3903 - val_loss: 12.0992\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 13.6144\n",
      "Set of Epoch 26-26\n",
      "\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.2953 - val_loss: 12.2336\n",
      "14985/14985 [==============================] - 72s 5ms/step - loss: 12.2364 - val_loss: 11.5184\n",
      "14985/14985 [==============================] - 63s 4ms/step - loss: 12.2594 - val_loss: 13.0398\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.4383 - val_loss: 14.9911\n",
      "14985/14985 [==============================] - 62s 4ms/step - loss: 12.2335 - val_loss: 15.7834\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.2335 - val_loss: 13.6548\n",
      "14985/14985 [==============================] - 65s 4ms/step - loss: 12.2631 - val_loss: 13.5288\n",
      "9990/9990 [==============================] - 46s 5ms/step - loss: 12.2288 - val_loss: 12.0154\n",
      "3125/3125 [==============================] - 10s 3ms/step - loss: 13.4488\n",
      "Set of Epoch 27-27\n",
      "\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.2225 - val_loss: 12.4440\n",
      "14985/14985 [==============================] - 80s 5ms/step - loss: 12.1572 - val_loss: 12.3501\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 12.1544 - val_loss: 13.1807\n",
      "14985/14985 [==============================] - 82s 5ms/step - loss: 12.2606 - val_loss: 15.9113\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.1591 - val_loss: 15.9502\n",
      "14985/14985 [==============================] - 87s 6ms/step - loss: 12.1611 - val_loss: 13.9480\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.2069 - val_loss: 13.4132\n",
      "9990/9990 [==============================] - 47s 5ms/step - loss: 12.1963 - val_loss: 11.9452\n",
      "3125/3125 [==============================] - 10s 3ms/step - loss: 13.7654\n",
      "Set of Epoch 28-28\n",
      "\n",
      "14985/14985 [==============================] - 68s 5ms/step - loss: 12.2231 - val_loss: 13.1241\n",
      "14985/14985 [==============================] - 73s 5ms/step - loss: 12.1012 - val_loss: 12.0591\n",
      "14985/14985 [==============================] - 64s 4ms/step - loss: 12.1826 - val_loss: 13.0021\n",
      "14985/14985 [==============================] - 83s 6ms/step - loss: 12.1885 - val_loss: 16.2607\n",
      "14985/14985 [==============================] - 70s 5ms/step - loss: 12.1109 - val_loss: 15.0640\n",
      "14985/14985 [==============================] - 77s 5ms/step - loss: 12.1313 - val_loss: 14.5181\n",
      "14985/14985 [==============================] - 67s 4ms/step - loss: 12.2183 - val_loss: 13.0562\n",
      "9990/9990 [==============================] - 44s 4ms/step - loss: 12.1287 - val_loss: 11.1067\n"
     ]
    }
   ],
   "source": [
    "ep=1\n",
    "batch=20\n",
    "\n",
    "for j in range(0,43):\n",
    "    print('Set of Epoch '+str(1+ep*j)+'-'+str(ep*(j+1)))\n",
    "    print('')\n",
    "    \n",
    "    for i in range(0,8):\n",
    "        labels = np.load('data/labels'+str(i)+'.npy')\n",
    "        samples = np.load('data/samples'+str(i)+'.npy')\n",
    "        if i==7:\n",
    "            model.fit(x=samples[0:200000],y=labels[0:200000],validation_split=0.001,batch_size=batch,epochs=ep,shuffle=True,verbose=1)\n",
    "        else:\n",
    "            model.fit(x=samples,y=labels,validation_split=0.001,batch_size=batch,epochs=ep,shuffle=True,verbose=1)\n",
    "        gc.collect()\n",
    "        time.sleep(10)\n",
    "    \n",
    "    samples = np.load('data/samples7.npy')\n",
    "    labels = np.load('data/labels7.npy')\n",
    "    score=model.evaluate(samples[200001:300000],labels[200001:300000], verbose=1)\n",
    "    gc.collect()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(samples[200001:300000]).flatten()\n",
    "truth = labels[200001:300000].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x = predicted, y = truth)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim([np.min(predicted[predicted>0]),np.max(predicted)])\n",
    "ax.set_ylim([np.min(truth),np.max(truth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('predicted.csv', predicted, delimiter=',')\n",
    "#np.savetxt('truth.csv', truth, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
